{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"강화학습 실습_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNQjZia67edQKreOqmzUnRp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"rJWHEhyKne0y","colab_type":"code","colab":{}},"source":["#@title\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","\n","# 이산 상황이라 DiscreteEnv로 지정하는 듯\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5oo8E7seFst","colab_type":"code","colab":{}},"source":["def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n","  \"\"\"\n","  정책 평가를 위한 함수\n","\n","  policy: [S, A] 행렬\n","  env: OpenAI env. env.P는 환경의 transition 확률을 나타냄 (Probability의 P 인듯)\n","  env.nS: State의 개수 (number of State)\n","  env.nA: Action의 개수 (number of Action)\n","  discount_factor: 할인율 (Gamma 값) \n","  theta: evaluation 반복을 무시할 크기?\n","  \"\"\"\n","\n","  # 상태 가치 함수 초기화\n","  V = np.zeros(env.nS)\n","\n","  while True:\n","    # 상태 비교를 위한 값\n","    delta = 0\n","\n","    # 모든 상태에 대해서 확인\n","    for s in range(env.nS):\n","      # s 상태에 대한 가치함수 계산을 위한 값\n","      v = 0\n","\n","      # 가치함수 계산\n","      for a, action_prob in enumerate(policy[s]):\n","        for prob, next_state, reward, done in env.P[s][a]:\n","          v += action_prob * prob * (reward + discount_factor * V[next_state])\n","\n","      delta = max(delta, np.abs(v - V[s]))\n","      V[s] = v\n","    \n","    # 흡수 상태\n","    if delta < theta:\n","      break\n","\n","  return np.array(V)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1EJ0ckuQeFnl","colab_type":"code","colab":{}},"source":["def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n","  \"\"\"\n","  정책 향상을 위한 함수\n","\n","  env: openAI env\n","  policy_eval_fn: 정책 평가를 위해 사용하는 함수\n","  discount_factor: 할인율\n","  \"\"\"\n","\n","  def one_step_lookahead(state, V):\n","    \"\"\"\n","    Helper function\n","    state: 상태\n","    V: 상태 가치함수\n","    \"\"\"\n","    A = np.zeros(env.nA)\n","    for a in range(env.nA):\n","      for prob, next_state, reward, done in env.P[state][a]:\n","        A[a] += prob * (reward + discount_factor * V[next_state])\n","    \n","    return A\n","  \n","  # 모든 상태에서 모든 행동을 수행할 가능성이 동일함\n","  policy = np.ones([env.nS, env.nA]) / env.nA\n","\n","  while True:\n","    # 상태 가치 함수를 얻음\n","    V = policy_eval_fn(policy, env, discount_factor)\n","\n","    policy_stable = True\n","    \n","    for s in range(env.nS):\n","      # 현재 정책에서 수행할 action 선택\n","      chosen_a = np.argmax(policy[s])\n","\n","      # Greedy한 action 선택\n","      action_values = one_step_lookahead(s, V)\n","      best_a = np.argmax(action_values)\n","\n","      if chosen_a != best_a:\n","        policy_stable = False\n","      policy[s] = np.eye(env.nA)[best_a]\n","    \n","    if policy_stable:\n","      return policy, V\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZt3qduVmWEZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"outputId":"637a3e82-4cb9-4ec2-9639-ca95549bced0","executionInfo":{"status":"ok","timestamp":1589215414314,"user_tz":-540,"elapsed":1063,"user":{"displayName":"김혁진","photoUrl":"","userId":"12736086656754012646"}}},"source":["env = GridworldEnv()\n","policy, v = policy_improvement(env)\n","\n","print(\"정책 확률 분포:\")\n","print(policy)\n","print()\n","\n","print(\"형태를 변형한 정책 그리드 (0=위, 1=오른쪽, 2=아래, 3=왼쪽):\")\n","print(np.reshape(np.argmax(policy, axis=1), env.shape))\n","print()\n","\n","print(\"가치함수:\")\n","print(v)\n","print()\n","\n","print(\"형태를 변형한 가치함수 그리드:\")\n","print(v.reshape(env.shape))\n","print()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["정책 확률 분포:\n","[[1. 0. 0. 0.]\n"," [0. 0. 0. 1.]\n"," [0. 0. 0. 1.]\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [1. 0. 0. 0.]]\n","\n","형태를 변형한 정책 그리드 (0=위, 1=오른쪽, 2=아래, 3=왼쪽):\n","[[0 3 3 2]\n"," [0 0 0 2]\n"," [0 0 1 2]\n"," [0 1 1 0]]\n","\n","가치함수:\n","[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n","\n","형태를 변형한 가치함수 그리드:\n","[[ 0. -1. -2. -3.]\n"," [-1. -2. -3. -2.]\n"," [-2. -3. -2. -1.]\n"," [-3. -2. -1.  0.]]\n","\n"],"name":"stdout"}]}]}